# ğŸ­ Multimodal Emotion Detection System

A complete AI-based emotion recognition system that detects human emotions using:

- ğŸ§  Text (NLP)
- ğŸ¤ Speech (Audio Processing)
- ğŸ“· Face (Computer Vision)
- ğŸ”¥ Fusion Model (Majority Voting)

This project combines multiple deep learning models to produce a final emotion prediction using multimodal fusion.

---

## ğŸš€ Project Overview

This system predicts emotions from three different input modalities:

1. Facial Expression (Image / Webcam)
2. Speech Emotion (Audio File)
3. Text Emotion (Sentence Input)

Each model predicts independently, and a Majority Fusion model determines the final emotion.

---

## ğŸ§  Architecture

```
          Image  â”€â”€â–º CNN Model â”€â”€â–º Face Emotion
          Audio  â”€â”€â–º Speech Model â”€â”€â–º Speech Emotion
          Text   â”€â”€â–º NLP Model â”€â”€â–º Text Emotion
                                â”‚
                                â–¼
                    Majority Voting Fusion
                                â”‚
                                â–¼
                      ğŸ¯ Final Emotion
```

---

## ğŸ§© Modules Used

### ğŸ“· Face Emotion Module
- CNN-based deep learning model
- Trained on FER2013 dataset
- Converts image to grayscale
- Outputs emotion + confidence score

### ğŸ¤ Speech Emotion Module
- Extracts audio features (MFCC)
- Deep learning classifier
- Supports .wav files
- Outputs emotion + confidence score

### ğŸ§  Text Emotion Module
- TF-IDF Vectorizer
- Machine Learning classifier
- Preprocessed text input
- Outputs emotion + confidence score

### ğŸ”¥ Fusion Module
- Majority voting strategy
- Combines predictions from all three models
- Outputs final emotion

---

## ğŸ›  Tech Stack

- Python
- PyTorch
- Scikit-learn
- OpenCV
- Librosa
- NumPy
- Streamlit (Frontend Interface)

---

## ğŸ“ Project Structure

```
multimodal_emotion_system/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ face_module/
â”‚   â”œâ”€â”€ train_cnn.py
â”‚   â””â”€â”€ predict_face.py
â”‚
â”œâ”€â”€ speech_module/
â”‚   â”œâ”€â”€ train_speech.py
â”‚   â””â”€â”€ predict_speech.py
â”‚
â”œâ”€â”€ text_module/
â”‚   â”œâ”€â”€ train_text.py
â”‚   â””â”€â”€ predict_text.py
â”‚
â”œâ”€â”€ fusion_module/
â”‚   â””â”€â”€ fusion_predict.py
â”‚
â””â”€â”€ models/
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone Repository

```
git clone https://github.com/your-username/multimodal-emotion-detection-system.git
cd multimodal-emotion-detection-system
```

### 2ï¸âƒ£ Create Environment

```
conda create -n emotion_env python=3.9
conda activate emotion_env
```

### 3ï¸âƒ£ Install Requirements

```
pip install -r requirements.txt
```

---

## â–¶ï¸ Run Application

```
streamlit run app.py
```

Open in browser:

```
http://localhost:8501
```

---

## ğŸ¯ Features

- Real-time Webcam Emotion Detection
- Audio File Emotion Detection
- Text-based Emotion Detection
- Confidence Score Display
- Multimodal Fusion Prediction
- Clean Interactive UI

---

## ğŸ“Š Example Output

Face Emotion: Sad (22.91%)  
Speech Emotion: Happy (99.13%)  
Text Emotion: Sad (50.3%)  

ğŸ”¥ Final Emotion (Fusion): Sad  

---

## ğŸ“ˆ Future Improvements

- Real-time microphone recording
- Live emotion tracking dashboard
- Model accuracy improvements
- Deployment on cloud (AWS / GCP)

---

## ğŸ‘¨â€ğŸ’» Author

Likith Reddy  
B.Tech - Computer Science  
Aspiring AI & Python Developer  

---

## â­ If You Like This Project

Give it a star â­ on GitHub!
